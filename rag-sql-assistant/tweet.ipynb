{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 1 - main tweet\n",
    "\n",
    "üîé Let's build a RAG-SQL agent that intelligently routes queries to a Text-to-SQL engine or uses retrieval over documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 2\n",
    "\n",
    "Here's what our RAG-SQL Agent can do:\n",
    "\n",
    "- Answer questions from unstructured Wikipedia data using RAG\n",
    "- Query structured city statistics using Text-to-SQL\n",
    "- Automatically choose the right tool for each query\n",
    "\n",
    "Let's see how it works! üöÄ\n",
    "\n",
    "{Demo GIF showing the assistant in action}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 3\n",
    "\n",
    "Here's the system architecture:\n",
    "\n",
    "Tech stack:\n",
    "- @Llama_Index for orchestration\n",
    "- @Llama_Cloud for vector database and search\n",
    "- @OpenAI for response generation\n",
    "- @Streamlit for the UI\n",
    "\n",
    "![System Architecture Diagram](assets/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 4\n",
    "\n",
    "0Ô∏è‚É£ Get our API keys:\n",
    "\n",
    "- `platform.openai.com`\n",
    "- `cloud.llamaindex.ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 4\n",
    "\n",
    "1Ô∏è‚É£ Set up our knowledge sources:\n",
    "\n",
    "1Ô∏è. A SQL database with city statistics (population, state)\n",
    "\n",
    "2Ô∏è. Wikipedia PDFs about major US cities indexed in LlamaCloud\n",
    "\n",
    "Both will be queried through specialized tools selected by our router."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 5\n",
    "\n",
    "2Ô∏è‚É£ Let's set up our SQL database first. We'll create an in-memory SQLite database with information about major US cities:\n",
    "\n",
    "```python\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, insert\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "city_stats_table = Table(\n",
    "    \"city_stats\", metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"state\", String(16), nullable=False),\n",
    ")\n",
    "\n",
    "# Add sample data below\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 6\n",
    "\n",
    "3Ô∏è‚É£ Now we create a Text-to-SQL query engine using LlamaIndex that can translate natural language into SQL queries:\n",
    "\n",
    "```python\n",
    "from llama_index.core import SQLDatabase\n",
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "sql_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    tables=[\"city_stats\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 7\n",
    "\n",
    "4Ô∏è‚É£ For the RAG component, we use LlamaCloud to index and query uploaded Wikipedia articles about major US cities, which we uploaded under the hood to an index:\n",
    "\n",
    "```python\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "    name=os.getenv(\"LLAMA_CLOUD_INDEX\"),\n",
    "    project_name=os.getenv(\"LLAMA_CLOUD_PROJECT\"),\n",
    "    organization_id=os.getenv(\"LLAMA_CLOUD_ORG_ID\"),\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    ")\n",
    "\n",
    "llama_cloud_query_engine = index.as_query_engine()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 8\n",
    "\n",
    "5Ô∏è‚É£ Now for the magic part - we create QueryEngineTools for both our engines which will be exposed to a router agent later:\n",
    "\n",
    "```python\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,\n",
    "    description=\"Useful for translating natural language into SQL queries over the city_stats table (population/state of US cities)\",\n",
    "    name=\"sql_tool\"\n",
    ")\n",
    "\n",
    "llama_cloud_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=llama_cloud_query_engine,\n",
    "    description=\"Useful for answering semantic questions about certain cities in the US\",\n",
    "    name=\"llama_cloud_tool\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 9\n",
    "\n",
    "6Ô∏è‚É£ The heart of our system is the router agent that decides which tool to use based on question type. Let's look at how it is implemented:\n",
    "\n",
    "```python\n",
    "from llama_index.core.workflow import Workflow\n",
    "\n",
    "class RouterOutputAgentWorkflow(Workflow):\n",
    "\n",
    "    def __init__(self,\n",
    "        tools,\n",
    "        timeout,\n",
    "        llm,\n",
    "        chat_history,\n",
    "    ):\n",
    "\n",
    "        self.tools = tools\n",
    "        self.tools_dict = {tool.metadata.name: tool for tool in self.tools}\n",
    "        self.llm = llm or OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "        self.chat_history = chat_history or []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 10\n",
    "\n",
    "7Ô∏è‚É£ A critical moment happens when the router's `chat` method is invoked, leading to an OpenAI LLM choosing which tool to use:\n",
    "\n",
    "```python\n",
    "    ... # the rest of the class\n",
    "    @step()\n",
    "    async def chat(self, ev: InputEvent):\n",
    "\n",
    "        # Put msg into LLM with tools included\n",
    "        chat_res = await self.llm.achat_with_tools(\n",
    "            self.tools,\n",
    "            chat_history=self.chat_history,\n",
    "            allow_parallel_tool_calls=True\n",
    "        )\n",
    "\n",
    "        # Tool calls\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(chat_res, error_on_no_tool_call=False)\n",
    "        \n",
    "        ai_message = chat_res.message\n",
    "        self.chat_history.append(ai_message)\n",
    "\n",
    "        # No tool calls, return chat message.\n",
    "        if not tool_calls:\n",
    "            return StopEvent(result=ai_message.content)\n",
    "\n",
    "        return GatherToolsEvent(tool_calls=tool_calls)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 11\n",
    "\n",
    "8Ô∏è‚É£ Finally, we initialize an instance of the router, provide it with the tools we created and a user query:\n",
    "\n",
    "```python\n",
    "workflow = RouterOutputAgentWorkflow(\n",
    "    tools=[sql_tool, llama_cloud_tool], verbose=True, timeout=120\n",
    ")\n",
    "\n",
    "result = await workflow.run(\n",
    "    message=\"Which city has the highest population?\"\n",
    ")\n",
    "\n",
    "print(result)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "New York City has the highest population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a wrap! Find the full code for this project from our GitHub repositoryüëá\n",
    "\n",
    "{LINK TO THE PROJECT CODE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in:\n",
    "\n",
    "- Python üêç\n",
    "- Machine Learning ü§ñ\n",
    "- AI Engineering ‚öôÔ∏è\n",
    "\n",
    "Find me ‚Üí @akshay_pachaar\n",
    "\n",
    "Cheers! ü•Ç"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
