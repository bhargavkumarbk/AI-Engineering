{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Local MCP Client with LlamaIndex Agent\n",
    "\n",
    "This Jupyter notebook walks you through creating a **local MCP (Model Context Protocol) client** that can chat with a database through tools exposed by an MCP server—completely on your machine. Follow the cells in order for a smooth, self‑contained tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Architecture and Benefits of MCP Client & Agent Separation\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                  MCP Client & Agent: Separation of Concerns                  ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                              ║\n",
    "║  ┌──────────────┐        Tool Calls        ┌──────────────┐                  ║\n",
    "║  │   Agent      │ ───────────────────────► │  MCP Client  │                  ║\n",
    "║  │ (LLM+Logic)  │                          │ (Tool Proxy) │                  ║\n",
    "║  └──────────────┘   ◄────────────────────  └──────────────┘                  ║\n",
    "║         ▲        Tool Results/Responses           ▲                          ║\n",
    "║         │                                         │                          ║\n",
    "║   User Input/Chat                          MCP Server/Tools                  ║\n",
    "║                                                                              ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║ • The **Agent** (LLM+Logic) interprets user intent and decides which tools   ║\n",
    "║   to call, but does not know tool implementation details.                    ║\n",
    "║ • The **MCP Client** discovers, registers, and communicates with tools       ║\n",
    "║   exposed by the MCP server, abstracting away tool endpoints and protocols.  ║\n",
    "║ • This separation enables:                                                   ║\n",
    "║    - Maintainability: Add/update tools in MCP layer, not agent logic.        ║\n",
    "║    - Extensibility: Swap agent or tool backend independently.                ║\n",
    "║    - Security: Agent never has direct access to tool implementations.        ║\n",
    "║    - Transparency: All tool calls are logged for quality/debugging.          ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  Setup a local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Initialize the MCP client and build the agent\n",
    "Point the client at your local MCP server’s **SSE endpoint** (default shown below), and list the available tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tools = McpToolSpec(client=mcp_client) # you can also pass list of allowed tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_data Add new data to the people table.\n",
      "\n",
      "Args:\n",
      "    name (str): Name of the person.\n",
      "    age (int): Age of the person.\n",
      "    profession (str): Profession of the person.\n",
      "Returns:\n",
      "    bool: True if data was added successfully, False otherwise.\n",
      "Example:\n",
      "    >>> add_data('Roger Federer', 45, 'Professional Tennis Player')\n",
      "    True\n",
      "\n",
      "read_data Read data from the people table using a SQL SELECT query and return as a list of dicts.\n",
      "\n",
      "Args:\n",
      "    query (str, optional): SQL SELECT query. Defaults to \"SELECT * FROM people\".\n",
      "        Examples:\n",
      "        - \"SELECT * FROM people\"\n",
      "        - \"SELECT name, age FROM people WHERE age > 25\"\n",
      "        - \"SELECT * FROM people ORDER BY age DESC\"\n",
      "\n",
      "Returns:\n",
      "    list: List of dictionaries containing the query results.\n",
      "          For default query, dict format is {\"id\": ..., \"name\": ..., \"age\": ..., \"profession\": ...}\n",
      "\n",
      "Example:\n",
      "    >>> # Read all records\n",
      "    >>> read_data()\n",
      "    [\n",
      "        {\"id\": 1, \"name\": \"John Doe\", \"age\": 30, \"profession\": \"Engineer\"},\n",
      "        {\"id\": 2, \"name\": \"Alice Smith\", \"age\": 25, \"profession\": \"Developer\"}\n",
      "    ]\n",
      "    \n",
      "    >>> # Read with custom query\n",
      "    >>> read_data(\"SELECT name, profession FROM people WHERE age < 30\")\n",
      "    [\n",
      "        {\"name\": \"Alice Smith\", \"profession\": \"Developer\"}\n",
      "    ]\n",
      "\n",
      "delete_person_by_name Delete a record from the people table by name if it exists.\n",
      "\n",
      "Args:\n",
      "    name (str): The name of the person to delete.\n",
      "Returns:\n",
      "    bool: True if a record was deleted, False otherwise.\n",
      "Example:\n",
      "    >>> delete_person_by_name('John Doe')\n",
      "    True\n",
      "\n",
      "update_person Update a person's record in the people table by id. You can update name, age, and/or profession.\n",
      "\n",
      "Args:\n",
      "    id (int): The id of the person to update.\n",
      "    name (str, optional): The new name. Defaults to None (no change).\n",
      "    age (int, optional): The new age. Defaults to None (no change).\n",
      "    profession (str, optional): The new profession. Defaults to None (no change).\n",
      "Returns:\n",
      "    bool: True if a record was updated, False otherwise.\n",
      "Example:\n",
      "    >>> update_person(1, name='Rafael Nadal', age=41)\n",
      "    True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tools = await mcp_tools.to_tool_list_async()\n",
    "for tool in tools:\n",
    "    print(tool.metadata.name, tool.metadata.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Define the system prompt\n",
    "This prompt steers the LLM when it needs to decide how and when to call tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an AI assistant for Tool Calling.\n",
    "\n",
    "Before you help a user, you need to work with tools to interact with Our Database.\n",
    "\n",
    "Always use the available tools to answer user questions. Do not make up information or answer from memory—only use the results returned by the tools.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4  Helper function: `get_agent()`\n",
    "Creates a `FunctionAgent` wired up with the MCP tool list and your chosen LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.mcp import McpToolSpec\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "async def get_agent(tools: McpToolSpec):\n",
    "    tools = await tools.to_tool_list_async()\n",
    "    agent = FunctionAgent(\n",
    "        name=\"Agent\",\n",
    "        description=\"An agent that can work with Our Database software.\",\n",
    "        tools=tools,\n",
    "        llm = Settings.llm,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Helper function: `handle_user_message()`\n",
    "Streams intermediate tool calls (for transparency) and returns the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    ToolCallResult, \n",
    "    ToolCall)\n",
    "\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "async def handle_user_message(\n",
    "    message_content: str,\n",
    "    agent: FunctionAgent,\n",
    "    agent_context: Context,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    handler = agent.run(message_content, ctx=agent_context)\n",
    "    async for event in handler.stream_events():\n",
    "        if verbose and isinstance(event, ToolCall):\n",
    "            print(f\"Calling tool {event.tool_name} with kwargs {event.tool_kwargs}\")\n",
    "            # Print the actual tool call for LLM quality assessment\n",
    "            print(f\"[MCP AGENT TOOL CALL] Tool: {event.tool_name}, Arguments: {event.tool_kwargs}\")\n",
    "        elif verbose and isinstance(event, ToolCallResult):\n",
    "            print(f\"Tool {event.tool_name} returned {event.tool_output}\")\n",
    "    response = await handler\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6  Initialize the MCP client and build the agent\n",
    "Point the client at your local MCP server’s **SSE endpoint** (default shown below), build the agent, and setup agent context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "\n",
    "# get the agent\n",
    "agent = await get_agent(mcp_tool)\n",
    "\n",
    "# create the agent context\n",
    "agent_context = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your message (type 'exit' to quit):\n"
     ]
    }
   ],
   "source": [
    "# Run the agent!\n",
    "while True:\n",
    "    #user_input = input(\"Enter your message: (type exit to quit)\")\n",
    "    print(\"Enter your message (type 'exit' to quit):\")\n",
    "    user_input = input(\"> \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    print(\"User: \", user_input)\n",
    "    response = await handle_user_message(user_input, agent, agent_context, verbose=True)\n",
    "    print(\"Agent: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
